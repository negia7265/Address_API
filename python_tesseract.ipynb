{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1lMOrVmpmmC"
      },
      "outputs": [],
      "source": [
        "!sudo apt install tesseract-ocr\n",
        "!pip install pytesseract"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pytesseract\n",
        "import re\n",
        "import shutil\n",
        "import os\n",
        "import random\n",
        "from IPython.display import display\n",
        "try:\n",
        " from PIL import Image\n",
        "except ImportError:\n",
        " import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "from IPython.display import display, Image as IPImage"
      ],
      "metadata": {
        "id": "zWeOy8vEpnRV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get grayscale image\n",
        "def get_grayscale(image):\n",
        "    return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# noise removal\n",
        "def remove_noise(image):\n",
        "    return cv2.medianBlur(image,5)\n",
        "\n",
        "#thresholding\n",
        "def thresholding(image):\n",
        "    return cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
        "\n",
        "#opening - erosion followed by dilation\n",
        "def opening(image):\n",
        "    kernel = np.ones((5,5),np.uint8)\n",
        "    return cv2.morphologyEx(image, cv2.MORPH_OPEN, kernel)\n",
        "\n",
        "#canny edge detection\n",
        "def canny(image):\n",
        "    return cv2.Canny(image, 100, 200)\n",
        "\n",
        "# Apply adaptive thresholding\n"
      ],
      "metadata": {
        "id": "YQrxopv7t6gH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = cv2.imread('/content/uber_test4.PNG')"
      ],
      "metadata": {
        "id": "FIz5e_NmuQCY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gray = get_grayscale(img)\n",
        "# noise=remove_noise(gray) dont do it!\n",
        "# thresh = thresholding(gray) nahh output is not good!\n",
        "# opening = opening(gray)  dont use!\n",
        "# canny = canny(gray)   dont use!"
      ],
      "metadata": {
        "id": "0ky0CJ9zuKxO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inverted_binary_image = cv2.bitwise_not(gray)\n",
        "# Image.fromarray(inverted_binary_image).show()\n",
        "\n",
        "#inverting image color not helping"
      ],
      "metadata": {
        "id": "YAnh-qAjKHXs"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "img_pillow = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "# image_path_in_colab=\"/content/ocr_test4.PNG\"\n",
        "extractedInformation = pytesseract.image_to_string(img_pillow)\n",
        "print(extractedInformation)"
      ],
      "metadata": {
        "id": "-1Gl4Nm1pyLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Trying regular expression here**"
      ],
      "metadata": {
        "id": "IPRAG8uGCtvi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "address_regex = re.compile(r'\\b\\d{1,2}:\\d{2}\\|\\s*([^,]+),(\\d+),\\s*([^,]+),\\s*([^,]+),\\s*([^,]+),\\s*([^,]+),\\s*([^,]+ \\d{6}),\\s*([^,]+)\\b')\n",
        "# Extract addresses using the regular expression\n",
        "matches = address_regex.findall(extractedInformation)\n",
        "# Print the matched addresses\n",
        "for match in matches:\n",
        "    print(match)"
      ],
      "metadata": {
        "id": "AOxi92Nq9GoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(img_pillow)"
      ],
      "metadata": {
        "id": "Q_Ftr07kvFWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uber dy 2023\n",
        "\n",
        "Here's your receipt for your ride, Punit\n",
        "\n",
        "‘We hope you enjoyed your ride this moming.\n",
        "\n",
        "\n",
        "\n",
        "Total 1969.00\n",
        "‘Tipcharge z198954\n",
        "Subtotal 1989.54\n",
        "BIAL Airport South Toll 11550\n",
        "Rounding 40.28\n",
        "Promotion 213576\n",
        "Payments\n",
        "\n",
        "Cash\n",
        "\n",
        "\n",
        "\n",
        "06/07/2023 12:42\n",
        "Visitthe trio nage for more information, including invoices (where available)\n",
        "\n",
        "‘The total of €1969.00 has a GST of 37.02 included.\n",
        "\n",
        "‘Yourode with AKASH\n",
        "License Plat: KAO2AJO4B6\n",
        "\n",
        "Uber Go Sedan 41.37 kilometres | 1h 18\n",
        "rmin(s)\n",
        "1 11:23| The Forum Mall,21, Hosur Rd, Chikku Lakshmaiah Layout, Koramangala, Bengaluru, Kamataka 560095, India\n",
        "\n",
        "12:41 | Kempegowda International Airport Bengaluru, 2 Terminal, Bengaluru, Kamataka 560300, India\n",
        "\n",
        "Fares are inclusive of GST. Please download the tax invoice from the trip detail page for a full tax breakdown."
      ],
      "metadata": {
        "id": "j8om0W5zt0rU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Trying NER here**"
      ],
      "metadata": {
        "id": "rklLc8D6CdFh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Using Spacy**"
      ],
      "metadata": {
        "id": "G6ux4lVZDLft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "ix0sQSxoqQU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc=nlp(extractedInformation)\n",
        "for ent in doc.ents:\n",
        "  if ent.label_== \"DATE\":\n",
        "    temp=ent.text\n",
        "  print(ent.text, \"|\", ent.label_, \"|\", spacy.explain(ent.label_))"
      ],
      "metadata": {
        "id": "mZtfAYpaBqYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Using Bert Base NER**"
      ],
      "metadata": {
        "id": "8Kqam0AlEolq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WtksNudODaib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "i4RvgLj2CXXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "PQVQmnKUBzul"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
        "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "DFJXH77HCIfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "ner_results = nlp(extractedInformation)\n",
        "print(ner_results)"
      ],
      "metadata": {
        "id": "WGIrAYpvDDAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Using Bert-large-NER**"
      ],
      "metadata": {
        "id": "iJL0nnfeEz8N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
        "\n",
        "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
        "ner_results = nlp(extractedInformation)\n",
        "print(ner_results)"
      ],
      "metadata": {
        "id": "PnMGJrFwDiow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_address(ner_result):\n",
        "    addresses = []\n",
        "    current_address = []\n",
        "\n",
        "    for token in ner_result:\n",
        "        if token['entity'].startswith('B-LOC'):\n",
        "            if current_address:\n",
        "                addresses.append(\" \".join(current_address))\n",
        "                current_address = [token['word']]\n",
        "            else:\n",
        "                current_address.append(token['word'])\n",
        "        elif token['entity'].startswith('I-LOC'):\n",
        "            current_address.append(token['word'])\n",
        "\n",
        "    if current_address:\n",
        "        addresses.append(\" \".join(current_address))\n",
        "\n",
        "    return addresses\n",
        "\n",
        "# Extract addresses\n",
        "addresses = extract_address(ner_results)\n",
        "\n",
        "# Print the addresses\n",
        "for address in addresses:\n",
        "    print(address)"
      ],
      "metadata": {
        "id": "896L50j3Rtup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def correctTokens(address):\n",
        "  split_address=address.split()\n",
        "  tokens = split_address\n",
        "  # Combine '##' tokens with the preceding non-'##' element\n",
        "  combined_tokens = []\n",
        "  current_token = None\n",
        "\n",
        "  for token in tokens:\n",
        "      if token.startswith('##'):\n",
        "          if current_token is not None:\n",
        "              current_token += token[2:]  # Concatenate with the non-'##' part\n",
        "      else:\n",
        "          if current_token is not None:\n",
        "              combined_tokens.append(current_token)\n",
        "          current_token = token\n",
        "\n",
        "  # Add the last token if it exists\n",
        "  if current_token is not None:\n",
        "      combined_tokens.append(current_token)\n",
        "\n",
        "  # Replace the first non-'##' token with the combined tokens\n",
        "  if combined_tokens:\n",
        "      tokens[0] = combined_tokens[0]\n",
        "\n",
        "  # Remove '##' tokens from the list\n",
        "  tokens = [token for token in combined_tokens if not token.startswith('##')]\n",
        "\n",
        "  # Print the result\n",
        "  # print(tokens)\n",
        "  result_string = ' '.join(tokens)\n",
        "\n",
        "  # Print the result\n",
        "  # print(result_string)\n",
        "  return result_string"
      ],
      "metadata": {
        "id": "nV9XJs-cX3oJ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correctTokens(addresses[9])"
      ],
      "metadata": {
        "id": "y6mKQBr7ZdIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corrected_addresses = []\n",
        "\n",
        "# Iterate through the original addresses and store the corrected ones in the new list\n",
        "for address in addresses:\n",
        "    corrected_addresses.append(correctTokens(address))\n",
        "\n",
        "# Print the corrected addresses\n",
        "for address in corrected_addresses:\n",
        "    print(address)"
      ],
      "metadata": {
        "id": "ncduCCZQb35m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "indices_of_india = [i for i, x in enumerate(corrected_addresses) if \"India\" in x]\n",
        "\n",
        "# Check if at least two occurrences of \"India\" are found\n",
        "if len(indices_of_india) >= 2:\n",
        "    # Extract the substring from the beginning to the first occurrence of \"India\"\n",
        "    substring_before_first_india = corrected_addresses[:indices_of_india[0] + 1]\n",
        "\n",
        "    # Extract the substring from the first occurrence of \"India\" to the second occurrence of \"India\"\n",
        "    substring_between_indias = corrected_addresses[indices_of_india[0] + 1:indices_of_india[1] + 1]\n",
        "\n",
        "first_address = '\\n'.join(substring_before_first_india)\n",
        "second_address= '\\n'.join(substring_between_indias)\n",
        "\n",
        "# Print the combined string\n",
        "print(first_address)\n",
        "print(second_address)\n"
      ],
      "metadata": {
        "id": "MkkLMoS8fG4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A2Gl70M8frgi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}